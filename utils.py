"""
ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€ç”»é¢è¡¨ç¤ºä»¥å¤–ã®æ§˜ã€…ãªé–¢æ•°å®šç¾©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã™ã€‚
"""

############################################################
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®èª­ã¿è¾¼ã¿
############################################################
import os
import time
from dotenv import load_dotenv
import streamlit as st
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain.schema import HumanMessage
from langchain_openai import ChatOpenAI
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
import constants as ct
import json

############################################################
# è¨­å®šé–¢é€£
############################################################
load_dotenv()
DEBUG = False  # â† True ã«ã™ã‚‹ã¨ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¡¨ç¤º

############################################################
# é–¢æ•°å®šç¾©
############################################################

def get_llm_response(chat_message):
    llm = ChatOpenAI(model_name=ct.MODEL, temperature=ct.TEMPERATURE)

    question_generator_prompt = ChatPromptTemplate.from_messages([
        ("system", ct.SYSTEM_PROMPT_CREATE_INDEPENDENT_TEXT),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}")
    ])

    question_answer_template = (
        ct.SYSTEM_PROMPT_DOC_SEARCH if st.session_state.mode == ct.ANSWER_MODE_1
        else ct.SYSTEM_PROMPT_INQUIRY
    )

    question_answer_prompt = ChatPromptTemplate.from_messages([
        ("system", question_answer_template),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}")
    ])

    history_aware_retriever = create_history_aware_retriever(
        llm, st.session_state.retriever, question_generator_prompt
    )
    question_answer_chain = create_stuff_documents_chain(llm, question_answer_prompt)
    chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

    if DEBUG:
        st.write("ğŸš© chain.invoke å®Ÿè¡Œå‰")

    start_time = time.time()

    try:
        result = chain.invoke({
            "input": chat_message,
            "chat_history": st.session_state.chat_history
        })
    except Exception as e:
        st.error("âŒ chain.invokeä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
        st.exception(e)
        raise e

    if DEBUG:
        st.write("âœ… chain.invoke å®Ÿè¡Œå¾Œ")
        st.write("ğŸ•’ å®Ÿè¡Œæ™‚é–“:", round(time.time() - start_time, 2), "ç§’")

    if result is None:
        st.error("âŒ chain.invokeã®çµæœãŒ None ã§ã—ãŸã€‚")
        st.stop()

    # JSONå¤‰æ›ã§ãã‚‹ã‚ˆã†ã«æ•´å½¢
    def serialize_document(doc):
        return {
            "page_content": doc.page_content,
            "metadata": doc.metadata,
        }

    result_serializable = result.copy()
    if isinstance(result, dict) and "context" in result:
        result_serializable["context"] = [serialize_document(doc) for doc in result["context"]]

    if DEBUG:
        with st.expander("ğŸ§ª ãƒ‡ãƒãƒƒã‚°æƒ…å ±ï¼ˆé–‹ç™ºè€…å‘ã‘ï¼‰"):
            st.write("ğŸ“¦ resultã®å‹:", type(result))
            st.code(json.dumps(result_serializable, indent=2, ensure_ascii=False), language="json")

    docs = result.get("context", []) if isinstance(result, dict) else []
    if not isinstance(docs, list) or len(docs) == 0:
        raise ValueError("é–¢é€£ã™ã‚‹æ–‡æ›¸ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚PDFãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ãªã„ã‹ã€è³ªå•ã¨ä¸€è‡´ã™ã‚‹å†…å®¹ãŒå«ã¾ã‚Œã¦ã„ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

    answer_raw = result.get("answer", "å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
    answer_text = answer_raw.get("text", str(answer_raw)) if isinstance(answer_raw, dict) else str(answer_raw)

    st.session_state.chat_history.extend([
        HumanMessage(content=chat_message),
        HumanMessage(content=answer_text)
    ])

    sources = []
    for doc in result.get("context", []):
        metadata = doc.metadata
        source_path = metadata.get("source") or metadata.get("file_path") or "ä¸æ˜ãªå‡ºå…¸"
        page_number = metadata.get("page")
        source_info = {"source": source_path}
        if isinstance(page_number, int):
            source_info["page_number"] = page_number + 1
        sources.append(source_info)

    # âœ… UIã¨ã—ã¦ã‚·ãƒ³ãƒ—ãƒ«ãªå›ç­”ã ã‘è¿”ã™
    if st.session_state.mode == ct.ANSWER_MODE_1:
        return {
            "mode": ct.ANSWER_MODE_1,
            "main_message": answer_text,
            "main_file_path": sources[0]["source"] if sources else None,
            "main_page_number": sources[0].get("page_number") if sources else None,
            "sub_message": "ä»–ã®å‚è€ƒæ–‡æ›¸ï¼š",
            "sub_choices": sources[1:] if len(sources) > 1 else []
        }
    else:
        return {
            "mode": ct.ANSWER_MODE_2,
            "answer": answer_text,
            "message": "å‚è€ƒã«ã—ãŸæ–‡æ›¸ã®ä¸€è¦§ï¼š",
            "file_info_list": [
                f"{s['source']}ï¼ˆp.{s['page_number']}ï¼‰" if "page_number" in s else s["source"]
                for s in sources
            ]
        }
    

def build_error_message(message):
    """
    ã‚¨ãƒ©ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨å…±é€šã®ç®¡ç†è€…å•ã„åˆã‚ã›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’é€£çµã™ã‚‹
    """
    return "\n".join([message, ct.COMMON_ERROR_MESSAGE])